{
  "general.name": "fastvlm-0.5b",
  "general.architecture": "generic",
  "general.tokenizer": "none",
  "general.quantization_version": 1,
  "general.alignment": 32,
  "general.hf_hub_model_id": "apple/FastVLM-0.5B",
  "size.vocabulary": 151936,
  "size.context": 512,
  "size.embedding": 896,
  "size.feedforward": 4864,
  "architecture.num_decoders": 24,
  "architecture.num_heads": 14,
  "architecture.num_kv_heads": 2,
  "architecture.connector": "sequential_pre_layer_normalization",
  "architecture.gating": "gated",
  "operation.normalization": "RMS-norm",
  "operation.normalization_epsilon": 1e-06,
  "operation.activation": "SiLU",
  "operation.positional_embedding": "RoPE",
  "operation.rope_complex_organization": "SoA",
  "operation.rope_num_rotations": 64,
  "operation.rope_scaling": 1000000.0,
  "tensor.layer_name": "model.layers.(\\d+).",
  "tensor.embedding_token_weight": {
    "name": "model.embed_tokens.weight"
  },
  "tensor.attention_normalization_weight": {
    "name": "input_layernorm.weight"
  },
  "tensor.attention_q_weight": {
    "name": "self_attn.q_proj.weight",
    "transposed": true
  },
  "tensor.attention_q_bias": {
    "name": "self_attn.q_proj.bias"
  },
  "tensor.attention_k_weight": {
    "name": "self_attn.k_proj.weight",
    "transposed": true
  },
  "tensor.attention_k_bias": {
    "name": "self_attn.k_proj.bias"
  },
  "tensor.attention_v_weight": {
    "name": "self_attn.v_proj.weight",
    "transposed": true
  },
  "tensor.attention_v_bias": {
    "name": "self_attn.v_proj.bias"
  },
  "tensor.attention_output_weight": {
    "name": "self_attn.o_proj.weight",
    "transposed": true
  },
  "tensor.feedforward_normalization_weight": {
    "name": "post_attention_layernorm.weight"
  },
  "tensor.feedforward_gate_weight": {
    "name": "mlp.gate_proj.weight",
    "transposed": true
  },
  "tensor.feedforward_up_weight": {
    "name": "mlp.up_proj.weight",
    "transposed": true
  },
  "tensor.feedforward_output_weight": {
    "name": "mlp.down_proj.weight",
    "transposed": true
  },
  "tensor.output_normalization_weight": {
    "name": "model.norm.weight"
  },
  "tensor.output_weight": {
    "name": "lm_head.weight",
    "transposed": true
  }
}
